{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import NBER graph and add department data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import nltk.metrics as nm\n",
    "import editdistance\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getJELSubgraph generates the subgraph of G containing all papers with \n",
    "# JEL code 'jelcode'\n",
    "def getJELSubgraph(G, jelcode):\n",
    "    SG = nx.Graph()\n",
    "    sge = [e for e in G.edges(data=True) if jelcode in e[2]['jelcode']]\n",
    "    SG.add_edges_from(sge)\n",
    "    return SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# getBigJELSubgraph generates the subgraph of G containing all papers with \n",
    "# JEL category (first letter) 'jelcode'\n",
    "def getBigJELSubgraph(G, jelcode):\n",
    "    SG = nx.Graph()\n",
    "    sge = [e for e in G.edges(data=True) if jelcode in re.findall(\"[a-zA-Z]\", e[2]['jelcode'])]\n",
    "    SG.add_edges_from(sge)\n",
    "    return SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns the Mean all-pairs node connectivity of a JEL Code\n",
    "def getConnEst(G, jelcode):\n",
    "    return np.mean(nx.all_pairs_node_connectivity(getJELSubgraph(G, jelcode)).values()[0].values()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get set of JEL super-categories (just first letter)\n",
    "def getBigJELs(jels):\n",
    "    bigJels = map(lambda x: re.findall(\"[a-zA-Z]\",x)[0], jels)\n",
    "    return set(bigJels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load NBER Graph from File\n",
    "# NBER graph contains coauthorship edges annotated with JEL codes \n",
    "path = '../save/nber.graphml'\n",
    "G = nx.read_graphml(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get List of JEL codes\n",
    "e = G.edges(data=True)\n",
    "jels = set(','.join([x[2]['jelcode'] for x in e]).split(','))\n",
    "bigJels = getBigJELs(jels)\n",
    "\n",
    "# Get List of Nodes\n",
    "n = G.nodes()\n",
    "ndf = pd.DataFrame(n, columns=['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Merge in Departments (scraped from REPEC)\n",
    "department = pd.read_csv('../../../work_erica/EconIdeaNetwork/save/REPEC_Paper_Info.csv', delimiter=',')\n",
    "department = department.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "department['authorClean'] = department.Author.apply(lambda x: x.replace(',','').split(' '))\n",
    "department['authorClean'] = department['authorClean'].apply(lambda x: x[1] + x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HACK: Drop duplicates by taking first institution \n",
    "department = department.groupby('authorClean').first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Manually check for matches using Levenstein Distance (tolerance = 2)\n",
    "\n",
    "authorDF = []\n",
    "\n",
    "def getAuthorMatches(author,allAuthors):\n",
    "\n",
    "    matches = pd.DataFrame(allAuthors.author[allAuthors.author.apply(lambda x: editdistance.eval(x, author)) < 2])\n",
    "    matches['repecAuthor'] = author\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get list of Author Matches\n",
    "\n",
    "deptMatches = department.authorClean.apply(lambda x: getAuthorMatches(x,ndf))\n",
    "deptMatches = pd.concat(list(deptMatches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fix Author Errors in NBER by merging\n",
    "# Nodes with wrong names\n",
    "\n",
    "s = deptMatches.groupby('repecAuthor').size()\n",
    "nberAuthorErrors = s[s > 1].index\n",
    "\n",
    "for currAuthor in nberAuthorErrors:\n",
    "\n",
    "    prevAuthorNames = deptMatches[deptMatches.repecAuthor == currAuthor].author.values\n",
    "\n",
    "    # For each prev name, collect all edges\n",
    "    # then delete node\n",
    "\n",
    "    allPrevEdges = []\n",
    "    allPrevEdgeAttributes = []\n",
    "    \n",
    "    for prevName in prevAuthorNames:\n",
    "\n",
    "        prevEdges = G.edges(prevName, data=True)\n",
    "        prevEdges = [x[1] for x in prevEdges]\n",
    "        prevEdgeAttributes = [x[2] for x in prevEdges]\n",
    "\n",
    "        allPrevEdges = allPrevEdges + prevEdges\n",
    "        allPrevEdgeAttributes = allPrevEdgeAttributes + prevEdgeAttributes\n",
    "\n",
    "        G.remove_node(prevName)\n",
    "\n",
    "    # Create new node with full set of edges and attributes\n",
    "\n",
    "    for i,target in enumerate(allPrevEdges):\n",
    "        G.add_edge(prevAuthorNames[0], target, jelcode=allPrevEdgeAttributes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9698\n",
      "9659\n"
     ]
    }
   ],
   "source": [
    "print len(ndf)\n",
    "print len(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge Department Affiliations onto Author Matches\n",
    "authDept = pd.merge(deptMatches, department, how='left', left_on='repecAuthor', right_on='authorClean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic full-graph summary stats\n",
    "degree_dist = nx.degree_histogram(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize a list of subgraphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def getExternalDegreeDist(subgraph, graph):\n",
    "    degs = np.array(nx.degree(graph, subgraph.nodes()).values()) - \\\n",
    "    np.array(nx.degree(subgraph, subgraph.nodes()).values())\n",
    "    \n",
    "    return np.histogram(degs, bins=max(degs))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgExternalDegree(subgraph, graph):\n",
    "    gdegree = np.array(nx.degree(graph, subgraph.nodes()).values())\n",
    "    sgdegree = np.array(nx.degree(subgraph, subgraph.nodes()).values())\n",
    "    return np.mean(gdegree - sgdegree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summarize a set \n",
    "# For each graph in a list, compute Avg Degree, Avg Betweenness Centrality, \n",
    "# Number of Components,\n",
    "# Number of Nodes, Longest Shortest Path, Total # of Papers, \n",
    "# Avg # of Papers per author, % of Papers within Dept, Rank\n",
    "\n",
    "# For example, \"kind\" could be 'Institution', \"entries\" would be a list of institutions\n",
    "def summGraphList(kind, entries, subgraphList, graph):\n",
    "    df = pd.DataFrame()\n",
    "    df['id']        = entries\n",
    "    \n",
    "    df['Subgraph']  = df.id.apply(lambda x: subgraphList[x])\n",
    "    df['nNodes'] = df.Subgraph.apply(lambda x: len(x.nodes()))\n",
    "    df['nWithinEdges'] = df.Subgraph.apply(lambda x: len(x.edges()))\n",
    "    df['nExternalEdges'] = df.Subgraph.apply(lambda x: len(x.edges()))\n",
    "        \n",
    "    # degree distribution within the dept\n",
    "    df['AvgWithinDegree'] = df.Subgraph.apply(lambda x: \\\n",
    "                                         np.mean(nx.degree(x, x.nodes()).values()))\n",
    "\n",
    "    \n",
    "    df['WithinDegreeDist'] = df.Subgraph.apply(lambda x: nx.degree_histogram(x))\n",
    "    df['WithinDegreeSkewness'] = df.WithinDegreeDist.apply(lambda x: stats.skew(x))\n",
    "    \n",
    "    # avg degree outside the department\n",
    "    df['AvgExternalDegree'] = df.Subgraph.apply(lambda x: getAvgExternalDegree(x, graph))\n",
    "    \n",
    "    df['ExternalDegreeDist'] = df.Subgraph.apply(lambda x: getExternalDegreeDist(x, graph))\n",
    "    df['ExternalDegreeSkewness'] = df.ExternalDegreeDist.apply(lambda x: stats.skew(x))\n",
    "    \n",
    "    # Drop empty entries\n",
    "    df = df[df.nNodes != 0]\n",
    "\n",
    "    # Summary stats for connected components\n",
    "    df['nConnectedComponents'] = df.Subgraph.apply(lambda x: \\\n",
    "                                   len([c for c in nx.connected_components(x)]))\n",
    "    df['AvgSizeConnectedComponent'] = df.Subgraph.apply(lambda x: \\\n",
    "                                         np.mean([len(c) for c in nx.connected_components(x)]))\n",
    "    df['MaxSizeConnectedComponent'] = df.Subgraph.apply(lambda x: \\\n",
    "                                         np.max([len(c) for c in nx.connected_components(x)]))\n",
    "  \n",
    "    # betweenness - gonna be very computationally intensive\n",
    "    # df['BetweennessDist'] = df.Graph.apply(lambda x: nx.betweenness_centrality(x))\n",
    "    \n",
    "    # clustering coefficients\n",
    "    df['clusteringCoeff'] = df.Subgraph.apply(lambda x: average_clustering(x))\n",
    "    \n",
    "    # rename id column \n",
    "    df.rename(columns={'id': kind})\n",
    "\n",
    "    # export to latex\n",
    "    text = df.to_latex()\n",
    "    fn = sprintf('../save/%s_summtable', kind)\n",
    "    f = open(fn, 'w')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine JEL subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save coauthorship network for each JEL category\n",
    "jelGraphs = {}\n",
    "overwrite = 1\n",
    "for currCode in bigJels:\n",
    "    jelGraphs[currCode] = getBigJELSubgraph(G, currCode)\n",
    "    if overwrite:\n",
    "        nx.write_graphml(jelGraphs[currCode], '../save/jel' + currCode + '.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sgl = summGraphList('bigJel', list(bigJels), jelGraphs, G)\n",
    "jel = 'A4'\n",
    "[e for e in G.edges(data=True) if jel in re.findall(\"[a-zA-Z]\", e[2]['jelcode'][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = [c for c in sgl.columns if 'id' not in c and 'Subgraph' not in c and 'Dist' not in c]\n",
    "nCols = len(cols)\n",
    "\n",
    "fig,ax = plt.subplots(nrows=nCols,ncols=nCols)\n",
    "\n",
    "for i in range(nCols):\n",
    "    for j in range(nCols):\n",
    "        print i,j\n",
    "        if i == j:\n",
    "            continue\n",
    "        else:\n",
    "            ax[i,j].scatter(sgl[cols[i]], sgl[cols[j]])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "edd = getExternalDegreeDist(jelGraphs['A'], G)\n",
    "type(edd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd = nx.degree_histogram(G) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(np.arange(len(dd)),dd)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#jeldf = summGraphList('bigJel',bigJels,jelGraphs, G)\n",
    "jelGraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jelGraphs['A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Examine institutional subgraphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get list of NBER authors corresponding to Institution\n",
    "\n",
    "instGraphs = {}\n",
    "insts = department.Institution.unique()\n",
    "\n",
    "# save coauthorship network for each institution\n",
    "for currInst in insts:\n",
    "    instGraphs[currInst] = G.subgraph(list(authDept[authDept.Institution == currInst].author))\n",
    "    nx.write_graphml(instGraphs[currInst], \\\n",
    "                     '../save/' + currInst.replace(',','').replace(' ','_') + '.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summarize Institutional Graphs\n",
    "\n",
    "instDf = pd.DataFrame()\n",
    "instDf['Institution'] = insts\n",
    "instDf['Graph'] = instDf.Institution.apply(lambda x: instGraphs[x])\n",
    "# Avg Degree, Avg Centrality, Number of Components, Number of Nodes, \n",
    "# Longest Shortest Path, Total # of Papers, Avg # of Papers, % of Papers within Dept\n",
    "# Rank\n",
    "\n",
    "instDf['AvgDegree'] = instDf.Graph.apply(lambda x: \\\n",
    "                                         np.mean(nx.degree(x, x.nodes()).values()))\n",
    "instDf['nNodes'] = instDf.Graph.apply(lambda x: len(x.nodes()))\n",
    "\n",
    "# Drop Empty Departments\n",
    "instDf = instDf[instDf.nNodes != 0]\n",
    "\n",
    "instDf['nCC'] = instDf.Graph.apply(lambda x: \\\n",
    "                                   len([c for c in nx.connected_components(x)]))\n",
    "instDf['AvgSizeCC'] = instDf.Graph.apply(lambda x: \\\n",
    "                                         np.mean([len(c) for c in nx.connected_components(x)]))\n",
    "instDf['MaxSizeCC'] = instDf.Graph.apply(lambda x: \\\n",
    "                                         np.max([len(c) for c in nx.connected_components(x)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NBERPapers = pd.read_csv('../save/NBER_Paper_Info.1.csv', delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netflix feature matrix\n",
    "\n",
    "- Pagerank\n",
    "- JEL codes\n",
    "- Geographic location (state, time zone) (prob time zone)\n",
    "- Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a JEL lookup table to map jels to indices                                                     \n",
    "jelLookup = dict()\n",
    "for i, jel in enumerate(jels):\n",
    "    jelLookup[jel] = i\n",
    "\n",
    "# make an array of authors x JELs \n",
    "# to count how many papers in each JEL someone has\n",
    "authors = nx.nodes(G)\n",
    "authorCodes = np.zeros((len(authors), len(jels)))\n",
    "for a in range(len(authors)):\n",
    "    papers = G.edges(authors[a])\n",
    "    for p in papers:\n",
    "        paperAttrs = G.get_edge_data(p[0], p[1])\n",
    "        paperJels  = paperAttrs['jelcode'].split(',')\n",
    "        jelInds = [jelLookup[jel] for jel in paperJels]\n",
    "        authorCodes[a, jelInds] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PageRanks\n",
    "ranks = nx.pagerank(G)\n",
    "\n",
    "# add to graph                                                                  \n",
    "nx.set_node_attributes(G, 'rank', ranks)\n",
    "\n",
    "# in case we want to look at the top authors                                    \n",
    "sortedRanks = sorted(ranks.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract PageRank to a vector                                                  \n",
    "authors = G.nodes()\n",
    "nodeRanks = np.empty(len(authors))\n",
    "for i in range(len(authors)):\n",
    "    nodeRanks[i] = ranks.get(authors[i])\n",
    "nodeRanks = np.reshape(nodeRanks, (len(nodeRanks),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make feature dataframe\n",
    "data = np.hstack((nodeRanks,authorCodes))\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['rank'] + [jel for jel in jels]\n",
    "df['author'] = authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G.nodes(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO \n",
    "\n",
    "- Aggregate JELs by letter \n",
    "- Add JELS as author attributes.\n",
    "- Look at degree distribution of subgraphs given by each JEL code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
